{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003e7199-cc0e-4282-bdd5-707ce7cdf9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "\n",
    "import extract_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70660c61-02ba-4151-828f-f8f346c64f34",
   "metadata": {},
   "source": [
    "## Extract Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0661fe2-0903-48f7-b8f3-aedec7f87f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    images_root = '/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/images'\n",
    "    features_dir = '/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/features/dino_vits16'\n",
    "    output_dir = '/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/eigs/laplacian'\n",
    "    which_matrix = 'laplacian'\n",
    "    which_color_matrix = 'knn'\n",
    "    which_features = 'k' # Again why just k features? Why not q and v?\n",
    "    normalize = True\n",
    "    threshold_at_zero = True\n",
    "    lapnorm = True\n",
    "    K = 20 # 20 classes for PASCAL VOC?\n",
    "    image_downsample_factor = None\n",
    "    image_color_lambda = 1.0 # Set to 0 if dense image features is not required\n",
    "    multiprocessing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dfe4719-0ad7-401b-a477-e85bf582450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "\n",
    "utils.make_output_dir(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22071b6-e875-4210-9a45-749e4f3cb7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K': 20, 'which_matrix': 'laplacian', 'which_features': 'k', 'which_color_matrix': 'knn', 'normalize': True, 'threshold_at_zero': True, 'images_root': '/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/images', 'output_dir': '/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/eigs/laplacian', 'image_downsample_factor': None, 'image_color_lambda': 1.0, 'lapnorm': True}\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary of arguments\n",
    "\n",
    "kwargs = dict(K=args.K,\n",
    "              which_matrix=args.which_matrix,\n",
    "              which_features=args.which_features,\n",
    "              which_color_matrix=args.which_color_matrix,\n",
    "              normalize=args.normalize,\n",
    "              threshold_at_zero=args.threshold_at_zero,\n",
    "              images_root=args.images_root,\n",
    "              output_dir=args.output_dir,\n",
    "              image_downsample_factor=args.image_downsample_factor,\n",
    "              image_color_lambda=args.image_color_lambda,\n",
    "              lapnorm=args.lapnorm)\n",
    "print (kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31d5f62-54bb-44cf-9a88-15b459b2b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = list(enumerate(sorted(Path(args.features_dir).iterdir())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "420ef716-37fe-47e0-aee9-46f1ce859295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, PosixPath('/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/features/dino_vits16/2007_000027.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Try out the Eigen vector operation with a single input from the inputs entries\n",
    "\n",
    "inp = inputs[0]\n",
    "print (inp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c11beb-d538-4963-b876-1948d64a438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigen(\n",
    "    inp: Tuple[int, str], \n",
    "    K: int, \n",
    "    images_root: str,\n",
    "    output_dir: str,\n",
    "    which_matrix: str = 'laplacian',\n",
    "    which_features: str = 'k',\n",
    "    normalize: bool = True,\n",
    "    lapnorm: bool = True,\n",
    "    which_color_matrix: str = 'knn',\n",
    "    threshold_at_zero: bool = True,\n",
    "    image_downsample_factor: Optional[int] = None,\n",
    "    image_color_lambda: float = 10,\n",
    "):\n",
    "    index, features_file = inp\n",
    "\n",
    "    # Load the features file\n",
    "    data_dict = torch.load(features_file, map_location='cpu')\n",
    "    image_id = data_dict['id']\n",
    "\n",
    "    # Load the output file storing the eigen vectors\n",
    "    output_file = str(Path(args.output_dir) / f'{image_id}.pth')\n",
    "    output_file_no_img = str(Path(args.output_dir) / f'{image_id}.pth')\n",
    "\n",
    "    if Path(output_file).is_file():\n",
    "        print (f'Skipping existing file {str(output_file)}')\n",
    "\n",
    "    # IMPORTANT: Load affinity matrix - Careful here\n",
    "    feats = data_dict[args.which_features].squeeze().cuda()\n",
    "    # print(feats.shape)\n",
    "\n",
    "    if args.normalize:\n",
    "        feats = F.normalize(feats, p=2, dim=-1)\n",
    "\n",
    "    if args.which_matrix in ['matting_laplacian', 'laplacian']:\n",
    "\n",
    "        # Get sizes\n",
    "        B, C, H, W, P, H_patch, W_patch, H_pad, W_pad = utils.get_image_sizes(data_dict)\n",
    "        if args.image_downsample_factor is None:\n",
    "            image_downsample_factor = P\n",
    "        H_pad_lr, W_pad_lr = H_pad // image_downsample_factor, W_pad // image_downsample_factor\n",
    "\n",
    "        # This probably didn't happen\n",
    "        if (H_patch, W_patch) != (H_pad_lr, W_pad_lr):\n",
    "            print ('This happened')\n",
    "            feats = F.interpolate(\n",
    "                feats.T.reshape(1, -1, H_patch, W_patch), \n",
    "                size=(H_pad_lr, W_pad_lr), mode='bilinear', align_corners=False\n",
    "            ).reshape(-1, H_pad_lr * W_pad_lr).T\n",
    "\n",
    "        ## Computing feature affinities\n",
    "        W_feat = (feats @ feats.T)\n",
    "        if args.threshold_at_zero:\n",
    "            W_feat = (W_feat * (W_feat > 0))\n",
    "        W_feat = W_feat / W_feat.max()\n",
    "        W_feat = W_feat.cpu().numpy()\n",
    "\n",
    "\n",
    "        ## Color affinities - If we are fusing with color affinities, then load the image and compute\n",
    "        if args.image_color_lambda > 0:\n",
    "            # Load image\n",
    "            image_file = str(Path(args.images_root) / f'{image_id}.jpg')\n",
    "            image_lr = Image.open(image_file).resize((W_pad_lr, H_pad_lr), Image.BILINEAR)\n",
    "            image_lr = np.array(image_lr) / 255.\n",
    "            # print (image_lr.shape)\n",
    "\n",
    "            # Color affinities\n",
    "            if args.which_color_matrix == 'knn':\n",
    "                W_lr = utils.knn_affinity(image_lr / 255)\n",
    "            elif args.which_color_matrix == 'rw':\n",
    "                W_lr = utils.knn_affinity(image_lr / 255)\n",
    "\n",
    "            # Convert to dense numpy array\n",
    "            W_color = np.array(W_lr.todense().astype(np.float32))\n",
    "\n",
    "        else:\n",
    "            W_color = 0\n",
    "\n",
    "        # Combine\n",
    "        W_comb = W_feat + W_color * args.image_color_lambda\n",
    "        D_comb = np.array(utils.get_diagonal(W_comb).todense())\n",
    "        D_comb_no_img = np.array(utils.get_diagonal(W_feat).todense())\n",
    "\n",
    "        # Extract eigenvectors\n",
    "        if args.lapnorm:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=args.K, sigma=0, which='LM', M=D_comb)\n",
    "                eigenvalues_no_img, eigenvectors_no_img = eigsh(D_comb_no_img - W_feat, k=args.K, sigma=0, which='LM', M=D_comb_no_img)\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=args.K, which='SM', M=D_comb)\n",
    "                eigenvalues_no_img, eigenvectors_no_img = eigsh(D_comb_no_img - W_feat, k=args.K, which='SM', M=D_comb)\n",
    "        else:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=args.K, sigma=0, which='LM')\n",
    "                eigenvalues_no_img, eigenvectors_no_img = eigsh(D_comb_no_img - W_feat, k=args.K, sigma=0, which='LM')\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=args.K, which='SM')\n",
    "                eigenvalues_no_img, eigenvectors_no_img = eigsh(D_comb_no_img - W_feat, k=args.K, which='SM')\n",
    "\n",
    "    eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\n",
    "    eigenvalues_no_img, eigenvectors_no_img = torch.from_numpy(eigenvalues_no_img), torch.from_numpy(eigenvectors_no_img.T).float()\n",
    "\n",
    "    # Sign ambiguity\n",
    "    for k in range(eigenvectors.shape[0]):\n",
    "        if 0.5 < torch.mean((eigenvectors[k] > 0).float()).item() < 1.0:  # reverse segment\n",
    "            eigenvectors[k] = 0 - eigenvectors[k]\n",
    "        if 0.5 < torch.mean((eigenvectors_no_img[k] > 0).float()).item() < 1.0:  # reverse segment\n",
    "            eigenvectors_no_img[k] = 0 - eigenvectors_no_img[k]\n",
    "\n",
    "    # Save dict\n",
    "    output_dict = {'eigenvalues': eigenvalues, 'eigenvectors': eigenvectors}\n",
    "    output_dict_no_img = {'eigenvalues': eigenvalues_no_img, 'eigenvectors': eigenvectors_no_img}\n",
    "\n",
    "    torch.save(output_dict, output_file)\n",
    "    torch.save(output_dict_no_img, output_file_no_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10740e42-e528-439d-892b-5d3cfb1bfb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 628/17125 [05:16<2:55:46,  1.56it/s] "
     ]
    }
   ],
   "source": [
    "fn = partial(compute_eigen, **kwargs)\n",
    "inputs = list(enumerate(sorted(Path(args.features_dir).iterdir())))\n",
    "utils.parallel_process(inputs, fn, args.multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c631be9-70c8-4012-879a-67d494d88f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Eigen values and Eigen vectors of affinity matrix by different methods\n",
    "\n",
    "# # Eigenvectors of affinity matrix\n",
    "# if which_matrix == 'affinity_torch':\n",
    "#     W = feats @ feats.T\n",
    "#     if threshold_at_zero:\n",
    "#         W = (W * (W > 0))\n",
    "#     eigenvalues, eigenvectors = torch.eig(W, eigenvectors=True)\n",
    "#     eigenvalues = eigenvalues.cpu()\n",
    "#     eigenvectors = eigenvectors.cpu()\n",
    "\n",
    "# # Eigenvectors of affinity matrix with scipy\n",
    "# elif which_matrix == 'affinity_svd':        \n",
    "#     USV = torch.linalg.svd(feats, full_matrices=False)\n",
    "#     eigenvectors = USV[0][:, :K].T.to('cpu', non_blocking=True)\n",
    "#     eigenvalues = USV[1][:K].to('cpu', non_blocking=True)\n",
    "\n",
    "# # Eigenvectors of affinity matrix with scipy\n",
    "# elif which_matrix == 'affinity':\n",
    "#     W = (feats @ feats.T)\n",
    "#     if threshold_at_zero:\n",
    "#         W = (W * (W > 0))\n",
    "#     W = W.cpu().numpy()\n",
    "#     eigenvalues, eigenvectors = eigsh(W, which='LM', k=K)\n",
    "#     eigenvectors = torch.flip(torch.from_numpy(eigenvectors), dims=(-1,)).T\n",
    "\n",
    "\n",
    "# # We implement the one using the Laplacian matrix but check the others too -- ABLATION!!\n",
    "# if args.which_matrix in ['matting_laplacian', 'laplacian']:\n",
    "    \n",
    "#     # Get sizes\n",
    "#     B, C, H, W, P, H_patch, W_patch, H_pad, W_pad = utils.get_image_sizes(data_dict)\n",
    "#     if args.image_downsample_factor is None:\n",
    "#         image_downsample_factor = P\n",
    "#     H_pad_lr, W_pad_lr = H_pad // image_downsample_factor, W_pad // image_downsample_factor\n",
    "    \n",
    "#     # This probably didn't happen\n",
    "#     if (H_patch, W_patch) != (H_pad_lr, W_pad_lr):\n",
    "#         print ('This happened')\n",
    "#         feats = F.interpolate(\n",
    "#             feats.T.reshape(1, -1, H_patch, W_patch), \n",
    "#             size=(H_pad_lr, W_pad_lr), mode='bilinear', align_corners=False\n",
    "#         ).reshape(-1, H_pad_lr * W_pad_lr).T\n",
    "    \n",
    "#     ## Computing feature affinities\n",
    "#     W_feat = (feats @ feats.T)\n",
    "#     if args.threshold_at_zero:\n",
    "#         W_feat = (W_feat * (W_feat > 0))\n",
    "#     W_feat = W_feat / W_feat.max()\n",
    "#     W_feat = W_feat.cpu().numpy()\n",
    "\n",
    "    \n",
    "#     ## Color affinities - If we are fusing with color affinities, then load the image and compute\n",
    "#     if args.image_color_lambda > 0:\n",
    "#         # Load image\n",
    "#         image_file = str(Path(args.images_root) / f'{image_id}.jpg')\n",
    "#         image_lr = Image.open(image_file).resize((W_pad_lr, H_pad_lr), Image.BILINEAR)\n",
    "#         image_lr = np.array(image_lr) / 255.\n",
    "#         print (image_lr.shape)\n",
    "    \n",
    "#         # Color affinities\n",
    "#         if args.which_color_matrix == 'knn':\n",
    "#             W_lr = utils.knn_affinity(image_lr / 255)\n",
    "#         elif args.which_color_matrix == 'rw':\n",
    "#             W_lr = utils.knn_affinity(image_lr / 255)\n",
    "        \n",
    "#         # Convert to dense numpy array\n",
    "#         W_color = np.array(W_lr.todense().astype(np.float32))\n",
    "        \n",
    "#     else:\n",
    "#         W_color = 0\n",
    "        \n",
    "#     # Combine\n",
    "#     W_comb = W_feat + W_color * args.image_color_lambda\n",
    "#     D_comb = np.array(utils.get_diagonal(W_comb).todense())\n",
    "#     D_comb_no_img = np.array(utils.get_diagonal(W_feat).todense())\n",
    "    \n",
    "#     # Extract eigenvectors\n",
    "#     if args.lapnorm:\n",
    "#         try:\n",
    "#             eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=args.K, sigma=0, which='LM', M=D_comb)\n",
    "#             eigenvalues_no_img, eigenvectors_no_img = eigsh(D_comb_no_img - W_feat, k=args.K, sigma=0, which='LM', M=D_comb_no_img)\n",
    "#         except:\n",
    "#             eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=args.K, which='SM', M=D_comb)\n",
    "#             eigenvalues_no_img, eigenvectors_no_img = eigsh(D_comb_no_img - W_feat, k=args.K, which='SM', M=D_comb)\n",
    "#     else:\n",
    "#         try:\n",
    "#             eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=args.K, sigma=0, which='LM')\n",
    "#             eigenvalues_no_img, eigenvectors_no_img = eigsh(D_comb_no_img - W_feat, k=args.K, sigma=0, which='LM')\n",
    "#         except:\n",
    "#             eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=args.K, which='SM')\n",
    "#             eigenvalues_no_img, eigenvectors_no_img = eigsh(D_comb_no_img - W_feat, k=args.K, which='SM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2fde88e-344e-412f-b5ee-f3660f922d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\n",
    "# eigenvalues_no_img, eigenvectors_no_img = torch.from_numpy(eigenvalues_no_img), torch.from_numpy(eigenvectors_no_img.T).float()\n",
    "\n",
    "# # Sign ambiguity\n",
    "# for k in range(eigenvectors.shape[0]):\n",
    "#     if 0.5 < torch.mean((eigenvectors[k] > 0).float()).item() < 1.0:  # reverse segment\n",
    "#         eigenvectors[k] = 0 - eigenvectors[k]\n",
    "#     if 0.5 < torch.mean((eigenvectors_no_img[k] > 0).float()).item() < 1.0:  # reverse segment\n",
    "#         eigenvectors_no_img[k] = 0 - eigenvectors_no_img[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2c47d74-9945-4fcc-a2f2-e8f74a1c1647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save dict\n",
    "# output_dict = {'eigenvalues': eigenvalues, 'eigenvectors': eigenvectors}\n",
    "# torch.save(output_dict, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43da111-9ac7-46fe-9d05-737c7d80c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_eigen()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
