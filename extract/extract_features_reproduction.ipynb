{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61587be-64ef-4502-baaf-c1f5530fd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "\n",
    "import extract_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4cb3ebe-1e69-40eb-a1fd-472dfbf85335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2907cc8-82ef-4a43-a8a9-4a82d371e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains inputs to the extract_features method\n",
    "class args:\n",
    "    images_list = '/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/lists/images.txt'\n",
    "    images_root = '/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/images'\n",
    "    output_dir = '/data/home/mukhotij/internship_2022/deep_spectral/data/VOC2012/features/dino_vits16'\n",
    "    model_name = 'dino_vits16'\n",
    "    batch_size = 1\n",
    "    which_block = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff92de2-2ef4-41f7-8f74-a009f48ed02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "\n",
    "utils.make_output_dir(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d2619e-0748-407b-bc2c-ef03d6d827a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data/home/mukhotij/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "model_name = args.model_name.lower()\n",
    "model, val_transform, patch_size, num_heads = utils.get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a9df8a-51bc-44bb-849f-dd6fb1a37837",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac818a3-9a2e-42dc-aeaa-452184dbc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a hook to capture features of the model (Could be useful for CLIP as well!!)\n",
    "\n",
    "if 'dino' in model_name or 'mocov3' in model_name:\n",
    "    feat_out = {}\n",
    "    def hook_fn_forward_qkv(module, input, output):\n",
    "        feat_out[\"qkv\"] = output\n",
    "    model._modules[\"blocks\"][args.which_block]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "else:\n",
    "    raise ValueError(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5353f154-3259-429b-a15f-edfd3e8c42e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: len(dataset)=17125\n",
      "Dataloader size: len(dataloader)=17125\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "\n",
    "filenames = Path(args.images_list).read_text().splitlines()\n",
    "dataset = utils.ImagesDataset(filenames=filenames, images_root=args.images_root, transform=val_transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, num_workers=8)\n",
    "print(f'Dataset size: {len(dataset)=}')\n",
    "print(f'Dataloader size: {len(dataloader)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e43a612-4d0a-42c1-beb6-0a1260573b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing model\n",
    "\n",
    "accelerator = Accelerator(fp16=True, cpu=False)\n",
    "model = model.to(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fbc95-e6a8-4832-9ee1-c93958c154f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/17125 [00:00<?, ?it/s]/data/home/mukhotij/miniconda/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/data/home/mukhotij/miniconda/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:3502: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "Processing:  45%|████▌     | 7751/17125 [10:08<12:40, 12.32it/s] "
     ]
    }
   ],
   "source": [
    "# Seems like there's no reshaping of the images here!!!! (This won't be the case in CLIP I guess!)\n",
    "\n",
    "pbar = tqdm(dataloader, desc='Processing')\n",
    "for i, (images, files, indices) in enumerate(pbar):\n",
    "    output_dict = {}\n",
    "    \n",
    "    # Check if file already exists\n",
    "    id = Path(files[0]).stem\n",
    "    \n",
    "    # Create output file for each image separately for storage\n",
    "    output_file = Path(args.output_dir) / f'{id}.pth'\n",
    "    if output_file.is_file():\n",
    "        pbar.write(f'Skipping existing file {str(output_file)}')\n",
    "        continue\n",
    "\n",
    "    # Reshape image (Pay very close attention to this part)\n",
    "    P = patch_size # 16 in the case of Vit-S/B-16\n",
    "    B, C, H, W = images.shape # Batch size, Channels, Height, Width - note that these are not reduced, same thing can be applied in case of CLIP.\n",
    "    # As long as the image can be patched up, it can be passed through the vision transformer - remember it is designed to deal with any number of tokens.\n",
    "    H_patch, W_patch = H // P, W // P # Number of patches along height and number of patches along width\n",
    "    H_pad, W_pad = H_patch * P, W_patch * P # The resizing dimensions which are exactly equal to the number of patches * patch size - we want to resize the image to this size\n",
    "    # T = number of tokens moving through the network\n",
    "    T = H_patch * W_patch + 1 # Additional one token for [CLS]\n",
    "\n",
    "    # Possible ablation: Bilinear interpolation\n",
    "    # images = F.interpolate(images, size=(H_pad, W_pad), mode='bilinear')  # resize image\n",
    "    images = images[:, :, :H_pad, :W_pad] # Simpler approach to above, just slice the image NOTE: This is where the resizing is happening. \n",
    "    # There is no resize operation which is necessary. This could be the same when applied to CLIP.\n",
    "    images = images.to(accelerator.device)\n",
    "    \n",
    "    \n",
    "    # Forward and collect features into the output dict\n",
    "    if 'dino' in model_name or 'mocov3' in model_name:\n",
    "        temp = model.get_intermediate_layers(images)[0].squeeze(0)\n",
    "        # print (feat_out[\"qkv\"].shape) # This has shape (B, T, num_heads * 3 * d_for_each_head(=64))\n",
    "        output_qkv = feat_out[\"qkv\"].reshape(B, T, 3, num_heads, -1 // num_heads).permute(2, 0, 3, 1, 4) # Reshaping to (B, T, 3, num_heads, d_for_each_head)\n",
    "        # Permuted shape is (3, B, num_heads, T, d_for_each_head)\n",
    "        \n",
    "        # output_dict['q'] = output_qkv[0].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "        # The above is first getting (B, num_heads, T, d_for_each_head)\n",
    "        # Then it is transposing and getting (B, T, num_heads, d_for_each_head)\n",
    "        # Then it is reshaping and getting (B, T, (num_heads * d_for_each_head(=384)))\n",
    "        # Then it is excluding the first token from the mix and taking only the rest of the tokens. (B, T-1, (num_heads * d_for_each_head))\n",
    "        # Similar operations are happening in the following two cases as well.\n",
    "        output_dict['k'] = output_qkv[1].transpose(1, 2).reshape(B, T, -1)[:, 1:, :] # Only storing the k part of the last attention layer of the vision transformer - why this in particular??\n",
    "        # output_dict['v'] = output_qkv[2].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "        \n",
    "    # Storing meta data\n",
    "    output_dict['indices'] = indices[0]\n",
    "    output_dict['file']  = files[0]\n",
    "    output_dict['id'] = id\n",
    "    output_dict['model_name'] = model_name\n",
    "    output_dict['patch_size'] = patch_size\n",
    "    output_dict['shape'] = (B, C, H, W)\n",
    "    output_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in output_dict.items()}\n",
    "    \n",
    "    # Saving the output_dict in the file\n",
    "    accelerator.save(output_dict, str(output_file))\n",
    "    accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a84d20-3daa-4fd1-9c33-c2659d1c8623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17124)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed82401-584b-46e5-b835-21b22e1bf6f4",
   "metadata": {},
   "source": [
    "## Random Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d335cc70-1cf1-4091-a859-f53c9264a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "\n",
    "def get_transform(name: str):\n",
    "    if any(x in name for x in ('dino', 'mocov3', 'convnext', )):\n",
    "        normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a5a3a00-edb0-420b-b7e1-8da81edda203",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = get_transform(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fc02677-4892-4fee-abc2-c745dbdb32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = model.patch_embed.patch_size\n",
    "num_heads = model.blocks[0].attn.num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0537a5e8-c12f-43d7-a129-ec1f39a916cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
